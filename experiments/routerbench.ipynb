{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/5cczn_jx4454n6fvhcg5qbjw0000gn/T/ipykernel_92285/1371555631.py:5: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  routerbench_data = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open and load the routerbench_5shot.pkl file\n",
    "with open('routerbench_5shot.pkl', 'rb') as f:\n",
    "    routerbench_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Configure pandas to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chinese_character_riddles', 'abstract2title', 'accounting_audit',\n",
       "       'arc-challenge', 'bias_detection', 'chinese-lantern-riddles',\n",
       "       'chinese-remainder-theorem',\n",
       "       'chinese_ancient_masterpieces_dynasty', 'chinese_ancient_poetry',\n",
       "       'chinese_chu_ci', 'chinese_famous_novel',\n",
       "       'chinese_hard_translations', 'chinese_homonym', 'chinese_idioms',\n",
       "       'chinese_modern_poem_identification', 'chinese_poem',\n",
       "       'chinese_shi_jing', 'chinese_song_ci', 'chinese_tang_poetries',\n",
       "       'chinese_zodiac', 'consensus_summary', 'grade-school-math',\n",
       "       'hellaswag', 'mbpp', 'mmlu-abstract-algebra', 'mmlu-anatomy',\n",
       "       'mmlu-astronomy', 'mmlu-business-ethics',\n",
       "       'mmlu-clinical-knowledge', 'mmlu-college-biology',\n",
       "       'mmlu-college-chemistry', 'mmlu-college-computer-science',\n",
       "       'mmlu-college-mathematics', 'mmlu-college-medicine',\n",
       "       'mmlu-college-physics', 'mmlu-computer-security',\n",
       "       'mmlu-conceptual-physics', 'mmlu-econometrics',\n",
       "       'mmlu-electrical-engineering', 'mmlu-elementary-mathematics',\n",
       "       'mmlu-formal-logic', 'mmlu-global-facts',\n",
       "       'mmlu-high-school-biology', 'mmlu-high-school-chemistry',\n",
       "       'mmlu-high-school-computer-science',\n",
       "       'mmlu-high-school-european-history', 'mmlu-high-school-geography',\n",
       "       'mmlu-high-school-government-and-politics',\n",
       "       'mmlu-high-school-macroeconomics', 'mmlu-high-school-mathematics',\n",
       "       'mmlu-high-school-microeconomics', 'mmlu-high-school-physics',\n",
       "       'mmlu-high-school-psychology', 'mmlu-high-school-statistics',\n",
       "       'mmlu-high-school-us-history', 'mmlu-high-school-world-history',\n",
       "       'mmlu-human-aging', 'mmlu-human-sexuality',\n",
       "       'mmlu-international-law', 'mmlu-jurisprudence',\n",
       "       'mmlu-logical-fallacies', 'mmlu-machine-learning',\n",
       "       'mmlu-management', 'mmlu-marketing', 'mmlu-medical-genetics',\n",
       "       'mmlu-miscellaneous', 'mmlu-moral-disputes',\n",
       "       'mmlu-moral-scenarios', 'mmlu-nutrition', 'mmlu-philosophy',\n",
       "       'mmlu-prehistory', 'mmlu-professional-accounting',\n",
       "       'mmlu-professional-law', 'mmlu-professional-medicine',\n",
       "       'mmlu-professional-psychology', 'mmlu-public-relations',\n",
       "       'mmlu-security-studies', 'mmlu-sociology',\n",
       "       'mmlu-us-foreign-policy', 'mmlu-virology', 'mmlu-world-religions',\n",
       "       'mtbench-math', 'mtbench-reference', 'mtbench', 'test-match',\n",
       "       'winogrande'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routerbench_data['eval_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>eval_name</th>\n",
       "      <th>WizardLM/WizardLM-13B-V1.2</th>\n",
       "      <th>claude-instant-v1</th>\n",
       "      <th>claude-v1</th>\n",
       "      <th>claude-v2</th>\n",
       "      <th>gpt-3.5-turbo-1106</th>\n",
       "      <th>gpt-4-1106-preview</th>\n",
       "      <th>meta/code-llama-instruct-34b-chat</th>\n",
       "      <th>meta/llama-2-70b-chat</th>\n",
       "      <th>mistralai/mistral-7b-chat</th>\n",
       "      <th>mistralai/mixtral-8x7b-chat</th>\n",
       "      <th>zero-one-ai/Yi-34B-Chat</th>\n",
       "      <th>gpt-3.5-turbo-1106|model_response</th>\n",
       "      <th>claude-instant-v1|model_response</th>\n",
       "      <th>claude-v1|model_response</th>\n",
       "      <th>claude-v2|model_response</th>\n",
       "      <th>gpt-4-1106-preview|model_response</th>\n",
       "      <th>meta/llama-2-70b-chat|model_response</th>\n",
       "      <th>mistralai/mixtral-8x7b-chat|model_response</th>\n",
       "      <th>zero-one-ai/Yi-34B-Chat|model_response</th>\n",
       "      <th>WizardLM/WizardLM-13B-V1.2|model_response</th>\n",
       "      <th>meta/code-llama-instruct-34b-chat|model_response</th>\n",
       "      <th>mistralai/mistral-7b-chat|model_response</th>\n",
       "      <th>gpt-3.5-turbo-1106|total_cost</th>\n",
       "      <th>claude-instant-v1|total_cost</th>\n",
       "      <th>claude-v1|total_cost</th>\n",
       "      <th>claude-v2|total_cost</th>\n",
       "      <th>gpt-4-1106-preview|total_cost</th>\n",
       "      <th>meta/llama-2-70b-chat|total_cost</th>\n",
       "      <th>mistralai/mixtral-8x7b-chat|total_cost</th>\n",
       "      <th>zero-one-ai/Yi-34B-Chat|total_cost</th>\n",
       "      <th>WizardLM/WizardLM-13B-V1.2|total_cost</th>\n",
       "      <th>meta/code-llama-instruct-34b-chat|total_cost</th>\n",
       "      <th>mistralai/mistral-7b-chat|total_cost</th>\n",
       "      <th>oracle_model_to_route_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21119</th>\n",
       "      <td>mmlu-abstract-algebra.val.0</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-abstract-algebra</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>['D\\n']</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>0.00536</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>WizardLM/WizardLM-13B-V1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21120</th>\n",
       "      <td>mmlu-abstract-algebra.val.1</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-abstract-algebra</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['C']</td>\n",
       "      <td>['A)']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['C']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.00549</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>WizardLM/WizardLM-13B-V1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21121</th>\n",
       "      <td>mmlu-abstract-algebra.val.10</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-abstract-algebra</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['D\\n']</td>\n",
       "      <td>['D\\n']</td>\n",
       "      <td>['C']</td>\n",
       "      <td>['D\\n']</td>\n",
       "      <td>['D\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>0.00571</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>mistralai/mistral-7b-chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21122</th>\n",
       "      <td>mmlu-abstract-algebra.val.11</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-abstract-algebra</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['C']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.00549</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>mistralai/mistral-7b-chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21123</th>\n",
       "      <td>mmlu-abstract-algebra.val.12</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-abstract-algebra</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['A)']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['D\\n']</td>\n",
       "      <td>['C']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['D\\n']</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.00534</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>meta/code-llama-instruct-34b-chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35156</th>\n",
       "      <td>mmlu-world-religions.val.95</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-world-religions</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.00477</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>mistralai/mistral-7b-chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35157</th>\n",
       "      <td>mmlu-world-religions.val.96</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-world-religions</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.00461</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>mistralai/mistral-7b-chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35158</th>\n",
       "      <td>mmlu-world-religions.val.97</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-world-religions</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['A)']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.00456</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>no_model_correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35159</th>\n",
       "      <td>mmlu-world-religions.val.98</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-world-religions</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['B\\n']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>WizardLM/WizardLM-13B-V1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35160</th>\n",
       "      <td>mmlu-world-religions.val.99</td>\n",
       "      <td>['Please answer with the letter of the correct...</td>\n",
       "      <td>mmlu-world-religions</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['A)']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['D']</td>\n",
       "      <td>['A']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['C']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['C\\n']</td>\n",
       "      <td>['A\\n']</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>mistralai/mistral-7b-chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14042 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sample_id  \\\n",
       "21119   mmlu-abstract-algebra.val.0   \n",
       "21120   mmlu-abstract-algebra.val.1   \n",
       "21121  mmlu-abstract-algebra.val.10   \n",
       "21122  mmlu-abstract-algebra.val.11   \n",
       "21123  mmlu-abstract-algebra.val.12   \n",
       "...                             ...   \n",
       "35156   mmlu-world-religions.val.95   \n",
       "35157   mmlu-world-religions.val.96   \n",
       "35158   mmlu-world-religions.val.97   \n",
       "35159   mmlu-world-religions.val.98   \n",
       "35160   mmlu-world-religions.val.99   \n",
       "\n",
       "                                                  prompt  \\\n",
       "21119  ['Please answer with the letter of the correct...   \n",
       "21120  ['Please answer with the letter of the correct...   \n",
       "21121  ['Please answer with the letter of the correct...   \n",
       "21122  ['Please answer with the letter of the correct...   \n",
       "21123  ['Please answer with the letter of the correct...   \n",
       "...                                                  ...   \n",
       "35156  ['Please answer with the letter of the correct...   \n",
       "35157  ['Please answer with the letter of the correct...   \n",
       "35158  ['Please answer with the letter of the correct...   \n",
       "35159  ['Please answer with the letter of the correct...   \n",
       "35160  ['Please answer with the letter of the correct...   \n",
       "\n",
       "                   eval_name WizardLM/WizardLM-13B-V1.2 claude-instant-v1  \\\n",
       "21119  mmlu-abstract-algebra                        1.0               1.0   \n",
       "21120  mmlu-abstract-algebra                        1.0               0.0   \n",
       "21121  mmlu-abstract-algebra                        0.0               0.0   \n",
       "21122  mmlu-abstract-algebra                        1.0               0.0   \n",
       "21123  mmlu-abstract-algebra                        0.0               0.0   \n",
       "...                      ...                        ...               ...   \n",
       "35156   mmlu-world-religions                        1.0               1.0   \n",
       "35157   mmlu-world-religions                        1.0               0.0   \n",
       "35158   mmlu-world-religions                        0.0               0.0   \n",
       "35159   mmlu-world-religions                        1.0               0.0   \n",
       "35160   mmlu-world-religions                        0.0               1.0   \n",
       "\n",
       "      claude-v1 claude-v2 gpt-3.5-turbo-1106 gpt-4-1106-preview  \\\n",
       "21119       1.0       0.0                0.0                0.0   \n",
       "21120       1.0       0.0                0.0                1.0   \n",
       "21121       0.0       0.0                0.0                0.0   \n",
       "21122       0.0       0.0                0.0                1.0   \n",
       "21123       0.0       0.0                0.0                0.0   \n",
       "...         ...       ...                ...                ...   \n",
       "35156       0.0       1.0                1.0                1.0   \n",
       "35157       1.0       1.0                1.0                1.0   \n",
       "35158       0.0       0.0                0.0                0.0   \n",
       "35159       1.0       1.0                0.0                1.0   \n",
       "35160       0.0       0.0                1.0                1.0   \n",
       "\n",
       "      meta/code-llama-instruct-34b-chat meta/llama-2-70b-chat  \\\n",
       "21119                               1.0                   0.0   \n",
       "21120                               0.0                   0.0   \n",
       "21121                               0.0                   0.0   \n",
       "21122                               1.0                   0.0   \n",
       "21123                               1.0                   1.0   \n",
       "...                                 ...                   ...   \n",
       "35156                               1.0                   0.0   \n",
       "35157                               1.0                   1.0   \n",
       "35158                               0.0                   0.0   \n",
       "35159                               0.0                   1.0   \n",
       "35160                               0.0                   1.0   \n",
       "\n",
       "      mistralai/mistral-7b-chat mistralai/mixtral-8x7b-chat  \\\n",
       "21119                       0.0                         0.0   \n",
       "21120                       0.0                         0.0   \n",
       "21121                       1.0                         0.0   \n",
       "21122                       1.0                         0.0   \n",
       "21123                       0.0                         0.0   \n",
       "...                         ...                         ...   \n",
       "35156                       1.0                         1.0   \n",
       "35157                       1.0                         1.0   \n",
       "35158                       0.0                         0.0   \n",
       "35159                       0.0                         1.0   \n",
       "35160                       1.0                         0.0   \n",
       "\n",
       "      zero-one-ai/Yi-34B-Chat gpt-3.5-turbo-1106|model_response  \\\n",
       "21119                     1.0                             ['D']   \n",
       "21120                     0.0                             ['C']   \n",
       "21121                     0.0                             ['D']   \n",
       "21122                     1.0                             ['D']   \n",
       "21123                     1.0                             ['D']   \n",
       "...                       ...                               ...   \n",
       "35156                     0.0                             ['A']   \n",
       "35157                     1.0                             ['B']   \n",
       "35158                     0.0                             ['B']   \n",
       "35159                     0.0                             ['A']   \n",
       "35160                     0.0                             ['A']   \n",
       "\n",
       "      claude-instant-v1|model_response claude-v1|model_response  \\\n",
       "21119                            ['A']                    ['A']   \n",
       "21120                           ['A)']                    ['B']   \n",
       "21121                            ['A']                    ['D']   \n",
       "21122                            ['A']                    ['A']   \n",
       "21123                           ['A)']                    ['B']   \n",
       "...                                ...                      ...   \n",
       "35156                            ['A']                    ['B']   \n",
       "35157                            ['A']                    ['B']   \n",
       "35158                           ['A)']                    ['B']   \n",
       "35159                            ['A']                    ['B']   \n",
       "35160                           ['A)']                    ['B']   \n",
       "\n",
       "      claude-v2|model_response gpt-4-1106-preview|model_response  \\\n",
       "21119                    ['D']                             ['D']   \n",
       "21120                    ['D']                             ['B']   \n",
       "21121                    ['D']                             ['D']   \n",
       "21122                    ['C']                             ['B']   \n",
       "21123                    ['B']                             ['D']   \n",
       "...                        ...                               ...   \n",
       "35156                    ['A']                             ['A']   \n",
       "35157                    ['B']                             ['B']   \n",
       "35158                    ['B']                             ['B']   \n",
       "35159                    ['B']                             ['B']   \n",
       "35160                    ['D']                             ['A']   \n",
       "\n",
       "      meta/llama-2-70b-chat|model_response  \\\n",
       "21119                              ['C\\n']   \n",
       "21120                              ['C\\n']   \n",
       "21121                              ['D\\n']   \n",
       "21122                              ['C\\n']   \n",
       "21123                              ['C\\n']   \n",
       "...                                    ...   \n",
       "35156                              ['B\\n']   \n",
       "35157                              ['B\\n']   \n",
       "35158                              ['B\\n']   \n",
       "35159                              ['B\\n']   \n",
       "35160                              ['A\\n']   \n",
       "\n",
       "      mistralai/mixtral-8x7b-chat|model_response  \\\n",
       "21119                                    ['C\\n']   \n",
       "21120                                    ['C\\n']   \n",
       "21121                                    ['D\\n']   \n",
       "21122                                    ['A\\n']   \n",
       "21123                                    ['D\\n']   \n",
       "...                                          ...   \n",
       "35156                                    ['A\\n']   \n",
       "35157                                    ['B\\n']   \n",
       "35158                                    ['B\\n']   \n",
       "35159                                    ['B\\n']   \n",
       "35160                                    ['C\\n']   \n",
       "\n",
       "      zero-one-ai/Yi-34B-Chat|model_response  \\\n",
       "21119                                  ['A']   \n",
       "21120                                  ['C']   \n",
       "21121                                  ['C']   \n",
       "21122                                  ['B']   \n",
       "21123                                  ['C']   \n",
       "...                                      ...   \n",
       "35156                                  ['B']   \n",
       "35157                                  ['B']   \n",
       "35158                                  ['B']   \n",
       "35159                                  ['A']   \n",
       "35160                                  ['C']   \n",
       "\n",
       "      WizardLM/WizardLM-13B-V1.2|model_response  \\\n",
       "21119                                   ['A\\n']   \n",
       "21120                                   ['B\\n']   \n",
       "21121                                   ['D\\n']   \n",
       "21122                                   ['B\\n']   \n",
       "21123                                   ['A\\n']   \n",
       "...                                         ...   \n",
       "35156                                   ['A\\n']   \n",
       "35157                                   ['B\\n']   \n",
       "35158                                   ['B\\n']   \n",
       "35159                                   ['B\\n']   \n",
       "35160                                   ['C\\n']   \n",
       "\n",
       "      meta/code-llama-instruct-34b-chat|model_response  \\\n",
       "21119                                          ['A\\n']   \n",
       "21120                                          ['C\\n']   \n",
       "21121                                          ['D\\n']   \n",
       "21122                                          ['B\\n']   \n",
       "21123                                          ['C\\n']   \n",
       "...                                                ...   \n",
       "35156                                          ['A\\n']   \n",
       "35157                                          ['B\\n']   \n",
       "35158                                          ['B\\n']   \n",
       "35159                                          ['A\\n']   \n",
       "35160                                          ['C\\n']   \n",
       "\n",
       "      mistralai/mistral-7b-chat|model_response  gpt-3.5-turbo-1106|total_cost  \\\n",
       "21119                                  ['D\\n']                       0.000535   \n",
       "21120                                  ['A\\n']                       0.000548   \n",
       "21121                                  ['B\\n']                       0.000570   \n",
       "21122                                  ['B\\n']                       0.000548   \n",
       "21123                                  ['D\\n']                       0.000533   \n",
       "...                                        ...                            ...   \n",
       "35156                                  ['A\\n']                       0.000476   \n",
       "35157                                  ['B\\n']                       0.000460   \n",
       "35158                                  ['B\\n']                       0.000455   \n",
       "35159                                  ['A\\n']                       0.000464   \n",
       "35160                                  ['A\\n']                       0.000458   \n",
       "\n",
       "       claude-instant-v1|total_cost  claude-v1|total_cost  \\\n",
       "21119                      0.000429              0.004288   \n",
       "21120                      0.000442              0.004392   \n",
       "21121                      0.000457              0.004568   \n",
       "21122                      0.000439              0.004392   \n",
       "21123                      0.000430              0.004272   \n",
       "...                             ...                   ...   \n",
       "35156                      0.000382              0.003816   \n",
       "35157                      0.000369              0.003688   \n",
       "35158                      0.000367              0.003648   \n",
       "35159                      0.000372              0.003720   \n",
       "35160                      0.000370              0.003672   \n",
       "\n",
       "       claude-v2|total_cost  gpt-4-1106-preview|total_cost  \\\n",
       "21119              0.004288                        0.00536   \n",
       "21120              0.004392                        0.00549   \n",
       "21121              0.004568                        0.00571   \n",
       "21122              0.004392                        0.00549   \n",
       "21123              0.004272                        0.00534   \n",
       "...                     ...                            ...   \n",
       "35156              0.003816                        0.00477   \n",
       "35157              0.003688                        0.00461   \n",
       "35158              0.003648                        0.00456   \n",
       "35159              0.003720                        0.00465   \n",
       "35160              0.003672                        0.00459   \n",
       "\n",
       "       meta/llama-2-70b-chat|total_cost  \\\n",
       "21119                          0.000481   \n",
       "21120                          0.000493   \n",
       "21121                          0.000513   \n",
       "21122                          0.000493   \n",
       "21123                          0.000480   \n",
       "...                                 ...   \n",
       "35156                          0.000428   \n",
       "35157                          0.000414   \n",
       "35158                          0.000409   \n",
       "35159                          0.000418   \n",
       "35160                          0.000412   \n",
       "\n",
       "       mistralai/mixtral-8x7b-chat|total_cost  \\\n",
       "21119                                0.000321   \n",
       "21120                                0.000329   \n",
       "21121                                0.000342   \n",
       "21122                                0.000329   \n",
       "21123                                0.000320   \n",
       "...                                       ...   \n",
       "35156                                0.000286   \n",
       "35157                                0.000276   \n",
       "35158                                0.000273   \n",
       "35159                                0.000278   \n",
       "35160                                0.000275   \n",
       "\n",
       "       zero-one-ai/Yi-34B-Chat|total_cost  \\\n",
       "21119                            0.000427   \n",
       "21120                            0.000438   \n",
       "21121                            0.000455   \n",
       "21122                            0.000438   \n",
       "21123                            0.000426   \n",
       "...                                   ...   \n",
       "35156                            0.000380   \n",
       "35157                            0.000367   \n",
       "35158                            0.000363   \n",
       "35159                            0.000370   \n",
       "35160                            0.000366   \n",
       "\n",
       "       WizardLM/WizardLM-13B-V1.2|total_cost  \\\n",
       "21119                               0.000160   \n",
       "21120                               0.000164   \n",
       "21121                               0.000171   \n",
       "21122                               0.000164   \n",
       "21123                               0.000160   \n",
       "...                                      ...   \n",
       "35156                               0.000143   \n",
       "35157                               0.000138   \n",
       "35158                               0.000136   \n",
       "35159                               0.000139   \n",
       "35160                               0.000137   \n",
       "\n",
       "       meta/code-llama-instruct-34b-chat|total_cost  \\\n",
       "21119                                      0.000415   \n",
       "21120                                      0.000425   \n",
       "21121                                      0.000442   \n",
       "21122                                      0.000425   \n",
       "21123                                      0.000414   \n",
       "...                                             ...   \n",
       "35156                                      0.000369   \n",
       "35157                                      0.000357   \n",
       "35158                                      0.000353   \n",
       "35159                                      0.000360   \n",
       "35160                                      0.000355   \n",
       "\n",
       "       mistralai/mistral-7b-chat|total_cost           oracle_model_to_route_to  \n",
       "21119                              0.000107         WizardLM/WizardLM-13B-V1.2  \n",
       "21120                              0.000110         WizardLM/WizardLM-13B-V1.2  \n",
       "21121                              0.000114          mistralai/mistral-7b-chat  \n",
       "21122                              0.000110          mistralai/mistral-7b-chat  \n",
       "21123                              0.000107  meta/code-llama-instruct-34b-chat  \n",
       "...                                     ...                                ...  \n",
       "35156                              0.000095          mistralai/mistral-7b-chat  \n",
       "35157                              0.000092          mistralai/mistral-7b-chat  \n",
       "35158                              0.000091                   no_model_correct  \n",
       "35159                              0.000093         WizardLM/WizardLM-13B-V1.2  \n",
       "35160                              0.000092          mistralai/mistral-7b-chat  \n",
       "\n",
       "[14042 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routerbench_data.query('eval_name.str.startswith(\"mmlu\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_router = routerbench_data.query('eval_name.str.startswith(\"mmlu\")', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_router = mmlu_router.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the common start of all MMLU prompts\n",
    "# prompts = mmlu_router['prompt'].tolist()\n",
    "# common_prefix = prompts[0]\n",
    "\n",
    "# for prompt in prompts[1:]:\n",
    "#     # Find first differing character\n",
    "#     for i, (c1, c2) in enumerate(zip(common_prefix, prompt)):\n",
    "#         if c1 != c2:\n",
    "#             common_prefix = common_prefix[:i]\n",
    "#             break\n",
    "            \n",
    "# # Remove the common prefix from all prompts\n",
    "# mmlu_router['prompt'] = mmlu_router['prompt'].str[len(common_prefix):]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate prompts from mmlu_router\n",
    "mmlu_router = mmlu_router.drop_duplicates(subset='prompt', keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14042, 37)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu_router.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100 prompt-oracle pairs from mmlu_router\n",
    "sampled_data = mmlu_router[['prompt', 'oracle_model_to_route_to']].sample(n=100, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sample_id', 'prompt', 'eval_name', 'WizardLM/WizardLM-13B-V1.2',\n",
       "       'claude-instant-v1', 'claude-v1', 'claude-v2', 'gpt-3.5-turbo-1106',\n",
       "       'gpt-4-1106-preview', 'meta/code-llama-instruct-34b-chat',\n",
       "       'meta/llama-2-70b-chat', 'mistralai/mistral-7b-chat',\n",
       "       'mistralai/mixtral-8x7b-chat', 'zero-one-ai/Yi-34B-Chat',\n",
       "       'gpt-3.5-turbo-1106|model_response', 'claude-instant-v1|model_response',\n",
       "       'claude-v1|model_response', 'claude-v2|model_response',\n",
       "       'gpt-4-1106-preview|model_response',\n",
       "       'meta/llama-2-70b-chat|model_response',\n",
       "       'mistralai/mixtral-8x7b-chat|model_response',\n",
       "       'zero-one-ai/Yi-34B-Chat|model_response',\n",
       "       'WizardLM/WizardLM-13B-V1.2|model_response',\n",
       "       'meta/code-llama-instruct-34b-chat|model_response',\n",
       "       'mistralai/mistral-7b-chat|model_response',\n",
       "       'gpt-3.5-turbo-1106|total_cost', 'claude-instant-v1|total_cost',\n",
       "       'claude-v1|total_cost', 'claude-v2|total_cost',\n",
       "       'gpt-4-1106-preview|total_cost', 'meta/llama-2-70b-chat|total_cost',\n",
       "       'mistralai/mixtral-8x7b-chat|total_cost',\n",
       "       'zero-one-ai/Yi-34B-Chat|total_cost',\n",
       "       'WizardLM/WizardLM-13B-V1.2|total_cost',\n",
       "       'meta/code-llama-instruct-34b-chat|total_cost',\n",
       "       'mistralai/mistral-7b-chat|total_cost', 'oracle_model_to_route_to'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu_router.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   1%|          | 1/100 [00:00<01:30,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   2%|▏         | 2/100 [00:01<01:20,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   3%|▎         | 3/100 [00:02<01:24,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   4%|▍         | 4/100 [00:03<01:25,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   5%|▌         | 5/100 [00:04<01:23,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   6%|▌         | 6/100 [00:05<01:22,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   7%|▋         | 7/100 [00:06<01:35,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   8%|▊         | 8/100 [00:07<01:33,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   9%|▉         | 9/100 [00:08<01:27,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  10%|█         | 10/100 [00:09<01:31,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  11%|█         | 11/100 [00:10<01:20,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  12%|█▏        | 12/100 [00:10<01:15,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  13%|█▎        | 13/100 [00:11<01:18,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  14%|█▍        | 14/100 [00:12<01:09,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  15%|█▌        | 15/100 [00:13<01:09,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  16%|█▌        | 16/100 [00:14<01:14,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  17%|█▋        | 17/100 [00:15<01:22,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  18%|█▊        | 18/100 [00:16<01:20,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  19%|█▉        | 19/100 [00:17<01:14,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  20%|██        | 20/100 [00:18<01:12,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  21%|██        | 21/100 [00:18<01:05,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  22%|██▏       | 22/100 [00:20<01:19,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  23%|██▎       | 23/100 [00:21<01:23,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  24%|██▍       | 24/100 [00:23<01:31,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  25%|██▌       | 25/100 [00:24<01:33,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  26%|██▌       | 26/100 [00:25<01:30,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  27%|██▋       | 27/100 [00:26<01:25,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  28%|██▊       | 28/100 [00:27<01:15,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  29%|██▉       | 29/100 [00:28<01:16,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  30%|███       | 30/100 [00:29<01:06,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  31%|███       | 31/100 [00:30<01:08,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  32%|███▏      | 32/100 [00:31<01:07,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  33%|███▎      | 33/100 [00:32<01:05,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  34%|███▍      | 34/100 [00:32<00:59,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  35%|███▌      | 35/100 [00:33<00:56,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  36%|███▌      | 36/100 [00:34<00:58,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  37%|███▋      | 37/100 [00:35<00:58,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  38%|███▊      | 38/100 [00:36<00:59,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  39%|███▉      | 39/100 [00:37<00:51,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  40%|████      | 40/100 [00:38<01:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  41%|████      | 41/100 [00:39<00:58,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  42%|████▏     | 42/100 [00:40<00:50,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  43%|████▎     | 43/100 [00:41<00:50,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  44%|████▍     | 44/100 [00:41<00:46,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  45%|████▌     | 45/100 [00:42<00:43,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  46%|████▌     | 46/100 [00:43<00:42,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  47%|████▋     | 47/100 [00:44<00:44,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  48%|████▊     | 48/100 [00:45<00:43,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  49%|████▉     | 49/100 [00:47<01:04,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  50%|█████     | 50/100 [00:51<01:38,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  51%|█████     | 51/100 [00:51<01:20,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  52%|█████▏    | 52/100 [00:53<01:10,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  53%|█████▎    | 53/100 [00:53<00:59,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  54%|█████▍    | 54/100 [00:54<00:55,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  55%|█████▌    | 55/100 [00:55<00:48,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  56%|█████▌    | 56/100 [00:56<00:44,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  57%|█████▋    | 57/100 [00:57<00:42,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  58%|█████▊    | 58/100 [00:58<00:39,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  59%|█████▉    | 59/100 [00:59<00:37,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  60%|██████    | 60/100 [01:00<00:36,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  61%|██████    | 61/100 [01:00<00:35,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  62%|██████▏   | 62/100 [01:01<00:34,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  63%|██████▎   | 63/100 [01:02<00:35,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  64%|██████▍   | 64/100 [01:03<00:32,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  65%|██████▌   | 65/100 [01:04<00:30,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  66%|██████▌   | 66/100 [01:05<00:31,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  67%|██████▋   | 67/100 [01:06<00:29,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  68%|██████▊   | 68/100 [01:07<00:31,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  69%|██████▉   | 69/100 [01:08<00:29,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  70%|███████   | 70/100 [01:09<00:27,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  71%|███████   | 71/100 [01:10<00:26,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  72%|███████▏  | 72/100 [01:11<00:26,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  73%|███████▎  | 73/100 [01:11<00:23,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  74%|███████▍  | 74/100 [01:13<00:25,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  75%|███████▌  | 75/100 [01:14<00:24,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  76%|███████▌  | 76/100 [01:15<00:27,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  77%|███████▋  | 77/100 [01:16<00:25,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  78%|███████▊  | 78/100 [01:17<00:22,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  79%|███████▉  | 79/100 [01:18<00:21,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  80%|████████  | 80/100 [01:19<00:20,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  81%|████████  | 81/100 [01:20<00:20,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  82%|████████▏ | 82/100 [01:21<00:17,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  83%|████████▎ | 83/100 [01:22<00:15,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  84%|████████▍ | 84/100 [01:22<00:13,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  85%|████████▌ | 85/100 [01:24<00:14,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  86%|████████▌ | 86/100 [01:25<00:14,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  87%|████████▋ | 87/100 [01:26<00:12,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  88%|████████▊ | 88/100 [01:27<00:11,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  89%|████████▉ | 89/100 [01:27<00:10,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  90%|█████████ | 90/100 [01:28<00:08,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  91%|█████████ | 91/100 [01:29<00:08,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  92%|█████████▏| 92/100 [01:30<00:06,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  93%|█████████▎| 93/100 [01:31<00:06,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  94%|█████████▍| 94/100 [01:32<00:05,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  95%|█████████▌| 95/100 [01:33<00:04,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  96%|█████████▌| 96/100 [01:34<00:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  97%|█████████▋| 97/100 [01:35<00:03,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt-3.5-turbo-1106'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  98%|█████████▊| 98/100 [01:36<00:01,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-instant-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  99%|█████████▉| 99/100 [01:37<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 100/100 [01:38<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'claude-v1'\n",
      "\n",
      "Accuracy: 2.00% (2/100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# List of possible models\n",
    "model_options = \"\"\"\n",
    "'WizardLM/WizardLM-13B-V1.2','claude-instant-v1', 'claude-v1', 'claude-v2', 'gpt-3.5-turbo-1106', 'gpt-4-1106-preview', 'meta/code-llama-instruct-34b-chat','meta/llama-2-70b-chat', 'mistralai/mistral-7b-chat','mistralai/mixtral-8x7b-chat', 'zero-one-ai/Yi-34B-Chat',\n",
    "\"\"\"\n",
    "\n",
    "# Function to check if correct model is in response\n",
    "def check_response(response, correct_model):\n",
    "    return correct_model.lower() in response.lower()\n",
    "\n",
    "# Track accuracy\n",
    "correct_predictions = 0\n",
    "total = len(sampled_data)\n",
    "\n",
    "# Process each prompt-oracle pair\n",
    "for _, row in tqdm(sampled_data.iterrows(), total=total, desc=\"Processing prompts\"):\n",
    "    prompt = row['prompt']\n",
    "    correct_model = row['oracle_model_to_route_to']\n",
    "    \n",
    "    # Construct the prompt for GPT-4\n",
    "    gpt_prompt = f\"\"\"Given the following task/prompt, select the most appropriate model from the options below.\n",
    "\n",
    "Task: {prompt}\n",
    "\n",
    "Available models:\n",
    "{model_options}\n",
    "\n",
    "Please respond with just the name of the cheapest model that will solve the task. For example, for a very challening task, since only GPT4 may solve it, you should return 'gpt-4-1106-preview'. However, if the task is simple, you should return the cheapest model that will solve the task, which may be 'mistralai/mistral-7b-chat'. The smaller models, such as 'mistralai/mistral-7b-chat' are much cheaper than the larger models. Small models such as 'mistralai/mistral-7b-chat', 'WizardLM/WizardLM-13B-V1.2', or medium-size models like 'mistralai/mixtral-8x7b-chat' and 'meta/llama-2-70b-chat', will tend to be cheapest.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Get GPT-4's response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": gpt_prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Check if the response contains the correct model\n",
    "        print(response.choices[0].message.content)\n",
    "        if check_response(response.choices[0].message.content, correct_model):\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = correct_predictions / total\n",
    "print(f\"\\nAccuracy: {accuracy:.2%} ({correct_predictions}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split mmlu_router into train and test sets with fixed sizes\n",
    "train_data = mmlu_router.sample(n=350, random_state=42)\n",
    "test_data = mmlu_router.drop(train_data.index).sample(n=75, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate prompts from train and test data\n",
    "train_data = train_data.drop_duplicates(subset='prompt', keep='first')\n",
    "test_data = test_data.drop_duplicates(subset='prompt', keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 14/14 [00:01<00:00, 13.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from clustering import cluster_sentences\n",
    "\n",
    "# Extract sentences from train and test data\n",
    "train_sentences = train_data['prompt'].tolist()\n",
    "test_sentences = test_data['prompt'].tolist()\n",
    "\n",
    "# Combine train and test sentences\n",
    "all_sentences = train_sentences + test_sentences\n",
    "\n",
    "# Cluster all sentences together\n",
    "clusters = cluster_sentences(all_sentences, n_clusters=70, show_graph=False, model_name = 'all-distilroberta-v1')\n",
    "\n",
    "# Create a mapping of sentence to cluster ID\n",
    "sentence_to_cluster = {}\n",
    "for cluster_id, cluster_data in clusters.items():\n",
    "    # Each cluster contains list of [sentence, embedding] pairs, except last element which is avg embedding\n",
    "    for item in cluster_data[:-1]:  # Exclude the last item (average embedding)\n",
    "        sentence_to_cluster[item[0]] = cluster_id\n",
    "\n",
    "# Add cluster IDs to train and test dataframes\n",
    "train_data['cluster_id'] = train_data['prompt'].map(sentence_to_cluster)\n",
    "test_data['cluster_id'] = test_data['prompt'].map(sentence_to_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 14/14 [00:00<00:00, 20.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(425, 768)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from clustering import embed_sentence\n",
    "embeddings = np.array(embed_sentence(all_sentences, model_name = 'all-distilroberta-v1', batch_size = 32))\n",
    "np.unique(embeddings, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 100\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, max_iter = 4000, tol = 10e-4)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {i: [] for i in range(n_clusters)}\n",
    "i=0\n",
    "for sentence, label in zip(all_sentences, cluster_labels):\n",
    "    clusters[label].append([sentence,embeddings[i]])\n",
    "    i+=1\n",
    "\n",
    "for label in range(n_clusters):\n",
    "    mask = cluster_labels == label\n",
    "    cluster_embeddings = embeddings[mask]\n",
    "    avg_embedding = np.mean(cluster_embeddings, axis=0)\n",
    "    clusters[label].append(avg_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of sentence to cluster ID\n",
    "sentence_to_cluster = {}\n",
    "for cluster_id, cluster_data in clusters.items():\n",
    "    # Each cluster contains list of [sentence, embedding] pairs, except last element which is avg embedding\n",
    "    for item in cluster_data[:-1]:  # Exclude the last item (average embedding)\n",
    "        sentence_to_cluster[item[0]] = cluster_id\n",
    "\n",
    "# Add cluster IDs to train and test dataframes\n",
    "train_data['cluster_id'] = train_data['prompt'].map(sentence_to_cluster)\n",
    "test_data['cluster_id'] = test_data['prompt'].map(sentence_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_id\n",
       "29    11\n",
       "21     9\n",
       "9      9\n",
       "13     8\n",
       "33     7\n",
       "      ..\n",
       "61     1\n",
       "86     1\n",
       "54     1\n",
       "75     1\n",
       "83     1\n",
       "Name: count, Length: 99, dtype: int64"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['cluster_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 samples:\n",
      "[\"['Write a python function to find element at a given index after number of rotations.']\", \"['Write a python function to find the minimum number of rotations (greater than 0) required to get the same string.']\", \"['Write a function to rotate a given list by specified number of items to the right direction. https://www.geeksforgeeks.org/python-program-right-rotate-list-n/']\"]\n",
      "\n",
      "Cluster 2 samples:\n",
      "[\"['Write a function to get a colon of a tuple.']\", \"['Write a function to convert a tuple to a string.']\", \"['Write a function to extract only the rear index element of each string in the given tuple.']\"]\n"
     ]
    }
   ],
   "source": [
    "# Sample 3 prompts from cluster 1 and cluster 2\n",
    "cluster_1_samples = train_data[train_data['cluster_id'] == 37]['prompt'].sample(n=3, random_state=42)\n",
    "cluster_2_samples = train_data[train_data['cluster_id'] == 33]['prompt'].sample(n=3, random_state=42)\n",
    "\n",
    "print(\"Cluster 1 samples:\")\n",
    "print([prompt[-800:] for prompt in cluster_1_samples])\n",
    "print(\"\\nCluster 2 samples:\")\n",
    "print([prompt[-800:] for prompt in cluster_2_samples])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of prompts to their best performing agents\n",
    "prompt_to_agent = {}\n",
    "\n",
    "# Process train data\n",
    "for idx, row in train_data.iterrows():\n",
    "    prompt = row['prompt']\n",
    "    # Get the agent with highest score for this prompt\n",
    "    best_agent = row['oracle_model_to_route_to']\n",
    "    prompt_to_agent[prompt] = best_agent\n",
    "\n",
    "# Process test data \n",
    "for idx, row in test_data.iterrows():\n",
    "    prompt = row['prompt']\n",
    "    # Get the agent with highest score for this prompt\n",
    "    best_agent = row['oracle_model_to_route_to']\n",
    "    prompt_to_agent[prompt] = best_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent mapping:\n",
      "WizardLM/WizardLM-13B-V1.2: 0\n",
      "claude-instant-v1: 1\n",
      "claude-v1: 2\n",
      "claude-v2: 3\n",
      "gpt-3.5-turbo-1106: 4\n",
      "gpt-4-1106-preview: 5\n",
      "meta/code-llama-instruct-34b-chat: 6\n",
      "meta/llama-2-70b-chat: 7\n",
      "mistralai/mistral-7b-chat: 8\n",
      "mistralai/mixtral-8x7b-chat: 9\n",
      "no_model_correct: 10\n",
      "zero-one-ai/Yi-34B-Chat: 11\n"
     ]
    }
   ],
   "source": [
    "# Get unique agents from the data\n",
    "unique_agents = sorted(list(set(prompt_to_agent.values())))\n",
    "\n",
    "# Create mapping of agent names to indices\n",
    "agent_to_idx = {agent: idx for idx, agent in enumerate(unique_agents)}\n",
    "\n",
    "# Create list of agent indices in order\n",
    "agents = list(range(len(unique_agents)))\n",
    "\n",
    "print(\"Agent mapping:\")\n",
    "for agent, idx in agent_to_idx.items():\n",
    "    print(f\"{agent}: {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from routing import initialize\n",
    "\n",
    "agents = [i for i in range(12)]\n",
    "cluster_vectors, agent_vectors = initialize(clusters, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example cluster distributions:\n",
      "\n",
      "Cluster 13:\n",
      "Agent indices: [0, 8, 9]\n",
      "Probabilities: [0.5, 0.375, 0.125]\n",
      "\n",
      "Cluster 90:\n",
      "Agent indices: [6, 8, 9]\n",
      "Probabilities: [0.25, 0.5, 0.25]\n",
      "\n",
      "Cluster 12:\n",
      "Agent indices: [6, 10, 11]\n",
      "Probabilities: [0.25, 0.5, 0.25]\n"
     ]
    }
   ],
   "source": [
    "# Create ground truth distributions for each cluster\n",
    "ground_truth = {}\n",
    "\n",
    "# First get cluster_id -> prompt mappings\n",
    "cluster_prompts = {}\n",
    "for idx, row in train_data.iterrows():\n",
    "    cluster_id = row['cluster_id']\n",
    "    prompt = row['prompt']\n",
    "    if cluster_id not in cluster_prompts:\n",
    "        cluster_prompts[cluster_id] = []\n",
    "    cluster_prompts[cluster_id].append(prompt)\n",
    "\n",
    "# For each cluster, calculate agent distribution\n",
    "for cluster_id, prompts in cluster_prompts.items():\n",
    "    # Count occurrences of each agent\n",
    "    agent_counts = {i: 0 for i in range(12)}\n",
    "    for prompt in prompts:\n",
    "        if prompt in prompt_to_agent:\n",
    "            agent = prompt_to_agent[prompt]\n",
    "            agent_idx = agent_to_idx[agent]\n",
    "            agent_counts[agent_idx] += 1\n",
    "    \n",
    "    # Convert counts to probabilities\n",
    "    total = sum(agent_counts.values())\n",
    "    if total > 0:  # Avoid division by zero\n",
    "        probs = [agent_counts[i]/total for i in range(12)]\n",
    "        # Get indices of non-zero probabilities\n",
    "        nonzero_indices = [i for i, p in enumerate(probs) if p > 0]\n",
    "        nonzero_probs = [p for p in probs if p > 0]\n",
    "        ground_truth[cluster_id] = (nonzero_indices, nonzero_probs)\n",
    "\n",
    "print(\"Example cluster distributions:\")\n",
    "for cluster_id in list(ground_truth.keys())[:3]:\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"Agent indices: {ground_truth[cluster_id][0]}\")\n",
    "    print(f\"Probabilities: {ground_truth[cluster_id][1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example cluster distributions:\n",
      "\n",
      "Cluster 13:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 90:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 12:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 22:\n",
      "Max Probability: 0.75\n",
      "\n",
      "Cluster 2:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 81:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 68:\n",
      "Max Probability: 0.25\n",
      "\n",
      "Cluster 5:\n",
      "Max Probability: 0.5714285714285714\n",
      "\n",
      "Cluster 11:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 60:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 40:\n",
      "Max Probability: 0.6666666666666666\n",
      "\n",
      "Cluster 32:\n",
      "Max Probability: 0.16666666666666666\n",
      "\n",
      "Cluster 14:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 79:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 9:\n",
      "Max Probability: 0.4444444444444444\n",
      "\n",
      "Cluster 72:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 15:\n",
      "Max Probability: 0.25\n",
      "\n",
      "Cluster 36:\n",
      "Max Probability: 0.8\n",
      "\n",
      "Cluster 99:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 78:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 59:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 20:\n",
      "Max Probability: 0.6666666666666666\n",
      "\n",
      "Cluster 27:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 98:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 29:\n",
      "Max Probability: 0.36363636363636365\n",
      "\n",
      "Cluster 75:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 18:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 55:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 48:\n",
      "Max Probability: 0.25\n",
      "\n",
      "Cluster 28:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 57:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 94:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 62:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 16:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 35:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 76:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 89:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 91:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 8:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 96:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 82:\n",
      "Max Probability: 0.6666666666666666\n",
      "\n",
      "Cluster 6:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 80:\n",
      "Max Probability: 0.6\n",
      "\n",
      "Cluster 77:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 21:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 53:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 0:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 10:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 58:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 1:\n",
      "Max Probability: 0.6666666666666666\n",
      "\n",
      "Cluster 30:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 17:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 3:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 65:\n",
      "Max Probability: 0.6\n",
      "\n",
      "Cluster 4:\n",
      "Max Probability: 0.75\n",
      "\n",
      "Cluster 23:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 88:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 50:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 33:\n",
      "Max Probability: 0.2857142857142857\n",
      "\n",
      "Cluster 47:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 69:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 71:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 37:\n",
      "Max Probability: 0.25\n",
      "\n",
      "Cluster 25:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 19:\n",
      "Max Probability: 0.7142857142857143\n",
      "\n",
      "Cluster 26:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 38:\n",
      "Max Probability: 0.6666666666666666\n",
      "\n",
      "Cluster 51:\n",
      "Max Probability: 0.8\n",
      "\n",
      "Cluster 43:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 54:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 24:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 45:\n",
      "Max Probability: 0.6\n",
      "\n",
      "Cluster 70:\n",
      "Max Probability: 0.6666666666666666\n",
      "\n",
      "Cluster 34:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 52:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 84:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 92:\n",
      "Max Probability: 0.75\n",
      "\n",
      "Cluster 83:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 41:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 42:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 63:\n",
      "Max Probability: 0.4\n",
      "\n",
      "Cluster 73:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 86:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 61:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 93:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 87:\n",
      "Max Probability: 0.25\n",
      "\n",
      "Cluster 74:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 85:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 97:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 66:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 67:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 46:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 31:\n",
      "Max Probability: 0.3333333333333333\n",
      "\n",
      "Cluster 49:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 95:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 44:\n",
      "Max Probability: 0.5\n",
      "\n",
      "Cluster 39:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 64:\n",
      "Max Probability: 1.0\n",
      "\n",
      "Cluster 7:\n",
      "Max Probability: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Example cluster distributions:\")\n",
    "for cluster_id in list(ground_truth.keys()):\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    # print(f\"Agent indices: {ground_truth[cluster_id][0]}\")\n",
    "    print(f\"Max Probability: {max(ground_truth[cluster_id][1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighted average maximum probability: 0.506\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average of max probabilities\n",
    "total_weight = 0\n",
    "weighted_sum = 0\n",
    "\n",
    "for cluster_id in ground_truth.keys():\n",
    "    # Get number of prompts in this cluster as weight\n",
    "    cluster_weight = len(cluster_prompts[cluster_id])\n",
    "    max_prob = max(ground_truth[cluster_id][1])\n",
    "    \n",
    "    weighted_sum += max_prob * cluster_weight\n",
    "    total_weight += cluster_weight\n",
    "\n",
    "weighted_avg_max_prob = weighted_sum / total_weight\n",
    "print(f\"\\nWeighted average maximum probability: {weighted_avg_max_prob:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a version of ground_truth with full probability vectors (including zeros)\n",
    "ground_truth_format = {}\n",
    "for cluster_id, (nonzero_indices, nonzero_probs) in ground_truth.items():\n",
    "    full_probs = [0.0] * 12  # Initialize with zeros for all agents\n",
    "    for idx, prob in zip(nonzero_indices, nonzero_probs):\n",
    "        full_probs[idx] = prob\n",
    "    ground_truth_format[cluster_id] = (range(12), full_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "56",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[374], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrouting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m learn_embeddings\n\u001b[0;32m----> 3\u001b[0m trained_cluster_vectors, trained_agent_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mground_truth_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs/Projects/ariadne-routing/routing.py:58\u001b[0m, in \u001b[0;36mlearn_embeddings\u001b[0;34m(cluster_vectors, agent_vectors, ground_truth, epochs, learning_rate, temperature, alpha)\u001b[0m\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Get ground truth distribution\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m true_agents, true_probs \u001b[38;5;241m=\u001b[39m \u001b[43mground_truth\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcluster_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Create full probability distribution (zeros for non-top-4 agents)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m true_distribution \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n_agents)\n",
      "\u001b[0;31mKeyError\u001b[0m: 56"
     ]
    }
   ],
   "source": [
    "from routing import learn_embeddings\n",
    "\n",
    "trained_cluster_vectors, trained_agent_vectors = learn_embeddings(\n",
    "        cluster_vectors, \n",
    "        agent_vectors,  \n",
    "        ground_truth_format,\n",
    "        epochs = 10,\n",
    "        learning_rate= 0.1,\n",
    "        temperature = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (768) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[376], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m prompt_embedding \u001b[38;5;241m=\u001b[39m cluster_vectors[cluster_id]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Predict agent\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m pred_agent_idx \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrained_agent_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Check if prediction is correct\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_agent_idx \u001b[38;5;241m==\u001b[39m true_agent_idx:\n",
      "Cell \u001b[0;32mIn[376], line 11\u001b[0m, in \u001b[0;36mpredict_agent\u001b[0;34m(prompt_embedding, agent_vectors, temperature)\u001b[0m\n\u001b[1;32m      7\u001b[0m     prompt_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(prompt_embedding, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate similarities with all agents\u001b[39;00m\n\u001b[1;32m     10\u001b[0m similarities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                      \u001b[49m\u001b[43magent_vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_vec \u001b[38;5;129;01min\u001b[39;00m agent_vectors\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m     14\u001b[0m ])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get predicted agent index\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m similarities\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Extract embeddings and make predictions on test data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_agent(prompt_embedding, agent_vectors, temperature=0.1):\n",
    "    # Convert prompt embedding to tensor if it's not already\n",
    "    if not isinstance(prompt_embedding, torch.Tensor):\n",
    "        prompt_embedding = torch.tensor(prompt_embedding, dtype=torch.float32)\n",
    "    \n",
    "    # Calculate similarities with all agents\n",
    "    similarities = torch.stack([\n",
    "        F.cosine_similarity(prompt_embedding.unsqueeze(0),\n",
    "                          agent_vec.unsqueeze(0))\n",
    "        for agent_vec in agent_vectors.values()\n",
    "    ])\n",
    "    \n",
    "    # Get predicted agent index\n",
    "    return similarities.argmax().item()\n",
    "\n",
    "# Make predictions on test data\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    cluster_id = row.cluster_id\n",
    "    true_agent_idx = agent_to_idx[row.oracle_model_to_route_to]\n",
    "\n",
    "    # Get the prompt embedding (assuming it's the last element)\n",
    "    prompt_embedding = cluster_vectors[cluster_id]\n",
    "    \n",
    "    # Predict agent\n",
    "    pred_agent_idx = predict_agent(prompt_embedding, trained_agent_vectors)\n",
    "    \n",
    "    # Check if prediction is correct\n",
    "    if pred_agent_idx == true_agent_idx:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\nAccuracy on test data: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proportion of same-cluster pairs with matching eval_name: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Count how often rows in same cluster have same eval_name\n",
    "same_eval_name_count = 0\n",
    "total_pairs = 0\n",
    "\n",
    "for _, row in train_data.iterrows():\n",
    "    # Get all rows with same cluster_id\n",
    "    cluster_rows = train_data[train_data.cluster_id == row.cluster_id]\n",
    "    \n",
    "    # Skip if only 1 row in cluster\n",
    "    if len(cluster_rows) <= 1:\n",
    "        continue\n",
    "        \n",
    "    # Sample another row from same cluster\n",
    "    other_row = cluster_rows.sample(n=1).iloc[0]\n",
    "    \n",
    "    # Check if eval_names match\n",
    "    if row.eval_name == other_row.eval_name:\n",
    "        same_eval_name_count += 1\n",
    "    total_pairs += 1\n",
    "\n",
    "cluster_eval_name_similarity = same_eval_name_count / total_pairs\n",
    "print(f\"\\nProportion of same-cluster pairs with matching eval_name: {cluster_eval_name_similarity:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of (cluster_id, agent_id) pairs from training data\n",
    "cluster_agent_pairs = []\n",
    "for _, row in train_data.iterrows():\n",
    "    cluster_id = row.cluster_id\n",
    "    agent_idx = agent_to_idx[row.oracle_model_to_route_to]\n",
    "    cluster_agent_pairs.append((cluster_id, agent_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 231.5181\n",
      "Epoch 1, Loss: 186.8882\n",
      "Epoch 2, Loss: 165.4914\n",
      "Epoch 3, Loss: 150.1116\n",
      "Epoch 4, Loss: 140.0291\n",
      "Epoch 5, Loss: 133.8012\n",
      "Epoch 6, Loss: 130.1145\n",
      "Epoch 7, Loss: 127.9464\n",
      "Epoch 8, Loss: 126.6201\n",
      "Epoch 9, Loss: 125.7465\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_embeddings(cluster_vectors, agent_vectors, cluster_agent_pairs, \n",
    "                     epochs=10, learning_rate=0.05, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Train embeddings using softmax contrastive loss with positive and negative pairs.\n",
    "    Agent vectors include a learnable bias term, cluster vectors have fixed 0 bias term.\n",
    "    Adds frequency-based penalties to discourage overuse of common agents.\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(cluster_vectors.values()) + list(agent_vectors.values()),\n",
    "        lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Calculate agent frequencies\n",
    "    agent_counts = {}\n",
    "    total_pairs = len(cluster_agent_pairs)\n",
    "    for _, agent_id in cluster_agent_pairs:\n",
    "        agent_counts[agent_id] = agent_counts.get(agent_id, 0) + 1\n",
    "    agent_freqs = {agent_id: count/total_pairs for agent_id, count in agent_counts.items()}\n",
    "    \n",
    "    # Determine printing interval safely\n",
    "    print_interval = max(1, epochs // 10)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Process each cluster\n",
    "        for cluster_id, cluster_embedding in cluster_vectors.items():\n",
    "            # Find the positive agent for this cluster\n",
    "            positive_agent = None\n",
    "            for c, a in cluster_agent_pairs:\n",
    "                if c == cluster_id:\n",
    "                    positive_agent = a\n",
    "                    break\n",
    "            if positive_agent is None:\n",
    "                continue\n",
    "            \n",
    "            # Compute cosine similarities for all agents\n",
    "            similarities = []\n",
    "            agent_ids = []\n",
    "            for agent_id, agent_embedding in agent_vectors.items():\n",
    "                # Compute cosine similarity (returns a tensor of shape (1,))\n",
    "                sim = F.cosine_similarity(\n",
    "                    cluster_embedding.unsqueeze(0),\n",
    "                    agent_embedding.unsqueeze(0)\n",
    "                )\n",
    "                # Add frequency penalty\n",
    "                freq_penalty = agent_freqs.get(agent_id, 0) * 0.1  # Scale factor of 0.1\n",
    "                sim = sim - freq_penalty\n",
    "                similarities.append(sim)\n",
    "                agent_ids.append(agent_id)\n",
    "            \n",
    "            # Stack and apply temperature scaling\n",
    "            logits = torch.stack(similarities) / temperature  # shape: (num_agents,)\n",
    "            logits = logits.unsqueeze(0)  # shape: (1, num_agents)\n",
    "            \n",
    "            # Create target tensor (index of positive agent)\n",
    "            positive_index = agent_ids.index(positive_agent)\n",
    "            target = torch.tensor([positive_index], dtype=torch.long, device=logits.device)\n",
    "            \n",
    "            # Compute cross entropy loss\n",
    "            loss = F.cross_entropy(logits, target.unsqueeze(0))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Normalize all vectors with a small epsilon to avoid division by zero\n",
    "            # Don't normalize the bias terms\n",
    "            with torch.no_grad():\n",
    "                for vec in agent_vectors.values():\n",
    "                    # Only normalize the non-bias terms\n",
    "                    vec.data[:-1] = vec.data[:-1] / (vec.data.norm(p=2) + 1e-8)\n",
    "                for vec in cluster_vectors.values():\n",
    "                    # Only normalize the non-bias terms\n",
    "                    vec.data[:-1] = vec.data[:-1] / (vec.data[:-1].norm(p=2) + 1e-8)\n",
    "                    vec.data[-1] = 1.0  # Reset cluster bias term to 0\n",
    "                    \n",
    "        if epoch % print_interval == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    return cluster_vectors, agent_vectors\n",
    "\n",
    "# Example usage:\n",
    "# Assume `cluster_vectors`, `agent_vectors`, and `cluster_agent_pairs` are defined.\n",
    "trained_cluster_vectors, trained_agent_vectors = train_embeddings(\n",
    "    {key: nn.Parameter(torch.cat([torch.normal(0, 1, size=(63,)), torch.ones(1)])) for key in cluster_vectors}, \n",
    "    {key: nn.Parameter(torch.normal(0, 1, size=(64,))) for key in agent_vectors},\n",
    "    cluster_agent_pairs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "cluster_vecs = {key: nn.Parameter(torch.normal(0, 1, size=(64,))) for key in cluster_vectors}\n",
    "agent_vecs = {key: nn.Parameter(torch.normal(0, 1, size=(64,))) for key in agent_vectors}\n",
    "\n",
    "# Calculate weighted average of agent vectors for each cluster\n",
    "for cluster_id in cluster_vecs:\n",
    "    # Get the agent distribution for this cluster\n",
    "    cluster_data = [agent_id for cid, agent_id in cluster_agent_pairs if cid == cluster_id]\n",
    "    total_samples = len(cluster_data)\n",
    "    \n",
    "    # Calculate weights for each agent\n",
    "    agent_weights = {}\n",
    "    for agent_id in agent_vecs:\n",
    "        count = sum(1 for x in cluster_data if x == agent_id)\n",
    "        agent_weights[agent_id] = 0 if total_samples == 0 else count / total_samples\n",
    "    \n",
    "    # Compute weighted average\n",
    "    weighted_sum = torch.zeros(64)\n",
    "    weights = torch.tensor([agent_weights[agent_id] for agent_id in agent_vecs.keys()])\n",
    "    weights = F.softmax(weights / .01, dim=0)  # Apply temperature of 0.1 to logits\n",
    "    # print(weights)\n",
    "    for agent_id, weight in zip(agent_vecs.keys(), weights):\n",
    "        weighted_sum += weight * agent_vecs[agent_id]\n",
    "        \n",
    "    # Update cluster vector\n",
    "    cluster_vecs[cluster_id].data = weighted_sum\n",
    "\n",
    "trained_cluster_vectors, trained_agent_vectors = cluster_vecs, agent_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on test data: 28.00%\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings and make predictions on test data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_agent(prompt_embedding, agent_vectors, temperature=0.1):\n",
    "    # Convert prompt embedding to tensor if it's not already\n",
    "    if not isinstance(prompt_embedding, torch.Tensor):\n",
    "        prompt_embedding = torch.tensor(prompt_embedding, dtype=torch.float32)\n",
    "    \n",
    "    # Calculate logits for all agents\n",
    "    logits = torch.stack([\n",
    "        torch.matmul(prompt_embedding, agent_vec)\n",
    "        for agent_vec in agent_vectors.values()\n",
    "    ])\n",
    "    \n",
    "    # Apply temperature scaling and softmax\n",
    "    probabilities = F.softmax(logits / temperature, dim=0)\n",
    "    \n",
    "    # Get predicted agent index\n",
    "    return probabilities.argmax().item()\n",
    "\n",
    "# Make predictions on test data\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    cluster_id = row.cluster_id\n",
    "    true_agent_idx = agent_to_idx[row.oracle_model_to_route_to]\n",
    "\n",
    "    prompt_embedding = trained_cluster_vectors[cluster_id]\n",
    "    \n",
    "    # Predict agent\n",
    "    pred_agent_idx = predict_agent(prompt_embedding, trained_agent_vectors)\n",
    "    # print(pred_agent_idx)\n",
    "    # Check if prediction is correct\n",
    "    if pred_agent_idx == true_agent_idx:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\nAccuracy on test data: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0228, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.0466, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3517, grad_fn=<SelectBackward0>)\n",
      "tensor(0.5380, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.1804, grad_fn=<SelectBackward0>)\n",
      "tensor(0.8495, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0363, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.6738, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.1815, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.8435, grad_fn=<SelectBackward0>)\n",
      "tensor(-1.1876, grad_fn=<SelectBackward0>)\n",
      "tensor(1.4942, grad_fn=<SelectBackward0>)\n",
      "\n",
      "Agent selection frequencies in test data:\n",
      "Agent 0: 13.3% (10 samples)\n",
      "Agent 1: 1.3% (1 samples)\n",
      "Agent 2: 1.3% (1 samples)\n",
      "Agent 3: 1.3% (1 samples)\n",
      "Agent 4: 10.7% (8 samples)\n",
      "Agent 5: 1.3% (1 samples)\n",
      "Agent 6: 10.7% (8 samples)\n",
      "Agent 8: 37.3% (28 samples)\n",
      "Agent 9: 10.7% (8 samples)\n",
      "Agent 10: 12.0% (9 samples)\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    print(trained_agent_vectors[i][-1])\n",
    "# Count how often each agent is the best performer\n",
    "agent_counts = {}\n",
    "for _, row in test_data.iterrows():\n",
    "    true_agent_idx = agent_to_idx[row.oracle_model_to_route_to]\n",
    "    agent_counts[true_agent_idx] = agent_counts.get(true_agent_idx, 0) + 1\n",
    "\n",
    "# Calculate and print percentages\n",
    "total_samples = len(test_data)\n",
    "print(\"\\nAgent selection frequencies in test data:\")\n",
    "for agent_idx, count in sorted(agent_counts.items()):\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"Agent {agent_idx}: {percentage:.1f}% ({count} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of clusters where agent 8 is not the best choice: 116\n"
     ]
    }
   ],
   "source": [
    "# Count clusters where agent 8 is not the best choice\n",
    "not_best_count = 0\n",
    "unique_clusters = test_data['cluster_id'].unique()\n",
    "\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_data = test_data[test_data['cluster_id'] == cluster_id]\n",
    "    agent_performance = {}\n",
    "    \n",
    "    # Count performance for each agent in this cluster\n",
    "    for _, row in cluster_data.iterrows():\n",
    "        true_agent = agent_to_idx[row.oracle_model_to_route_to]\n",
    "        agent_performance[true_agent] = agent_performance.get(true_agent, 0) + 1\n",
    "    \n",
    "    # Find agent with best performance\n",
    "    best_agent = max(agent_performance.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    if best_agent != 8:\n",
    "        not_best_count += 1\n",
    "\n",
    "print(f\"\\nNumber of clusters where agent 8 is not the best choice: {not_best_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unique cluster, find the most frequent agent and set cluster embedding\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_data = test_data[test_data['cluster_id'] == cluster_id]\n",
    "    agent_performance = {}\n",
    "    \n",
    "    # Count performance for each agent in this cluster\n",
    "    for _, row in cluster_data.iterrows():\n",
    "        true_agent = agent_to_idx[row.oracle_model_to_route_to]\n",
    "        agent_performance[true_agent] = agent_performance.get(true_agent, 0) + 1\n",
    "    \n",
    "    # Find agent with best performance\n",
    "    best_agent = max(agent_performance.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    # Set cluster embedding to match the best performing agent's embedding\n",
    "    trained_cluster_vectors[cluster_id] = trained_agent_vectors[best_agent].clone()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
